# ğŸ¨ Web UI cho Vintern Vision Test

## âœ… GIáº¢I QUYáº¾T Váº¤N Äá»€

**Váº¥n Ä‘á» ban Ä‘áº§u:** Web UI máº·c Ä‘á»‹nh cá»§a llama.cpp **khÃ´ng há»— trá»£ upload áº£nh** cho multimodal models.

**Giáº£i phÃ¡p:** Táº¡o custom Web UI vá»›i:
- âœ… Upload/drag-drop áº£nh
- âœ… Preview áº£nh
- âœ… Nháº­p prompt tiáº¿ng Viá»‡t
- âœ… Hiá»ƒn thá»‹ response Ä‘áº¹p
- âœ… Show timing vÃ  token stats

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. Start cáº£ 2 services

```bash
cd /home/baobao/Projects/Vintern-1b-v3.5-demo

# Terminal 1: PC Inference Server (Ä‘Ã£ cháº¡y)
cd models/gguf
/home/baobao/Projects/llama.cpp-vintern/build/bin/llama-server \
  -m Vintern-1B-v3_5-Q8_0.gguf \
  --mmproj mmproj-Vintern-1B-v3_5-Q8_0.gguf \
  --host 0.0.0.0 \
  --port 8080

# Terminal 2: Web UI (Ä‘Ã£ cháº¡y)
cd web-ui
python3 -m http.server 3000
```

### 2. Má»Ÿ browser

```
http://localhost:3000
```

### 3. Test vá»›i áº£nh

1. **Click** vÃ o há»™p upload hoáº·c **kÃ©o tháº£** áº£nh
2. **Nháº­p** cÃ¢u há»i (máº·c Ä‘á»‹nh: "MÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y trong áº£nh nÃ y.")
3. **Click** "ğŸš€ PhÃ¢n tÃ­ch áº£nh"
4. **Äá»£i** 2-3s (CPU mode) hoáº·c ~0.5s (náº¿u cÃ³ GPU)
5. **Xem** káº¿t quáº£!

## ğŸ“Š Test Results

### Test vá»›i test-fruits.jpg
```
âœ… SUCCESS!
Response: "HÃ¬nh áº£nh chá»¥p cáº­n cáº£nh má»™t Ä‘á»‘ng trÃ¡i cÃ¢y,"
Time: ~2-3s
Tokens: 313
```

## ğŸ¯ Features

### Web UI Features
- âœ… Drag & drop upload
- âœ… Image preview
- âœ… Vietnamese prompt support
- âœ… Loading animation
- âœ… Error handling
- âœ… Response stats (time + tokens)
- âœ… Beautiful gradient UI
- âœ… Responsive design

### API Endpoints
```
POST http://localhost:8080/v1/chat/completions

Body:
{
  "messages": [
    {
      "role": "user",
      "content": [
        {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}},
        {"type": "text", "text": "Your prompt"}
      ]
    }
  ],
  "max_tokens": 200,
  "temperature": 0.1
}
```

## ğŸ”§ Troubleshooting

### Lá»—i "Failed to tokenize prompt"
**NguyÃªn nhÃ¢n:** Server chÆ°a load mmproj Ä‘Ãºng, hoáº·c sai format

**Giáº£i phÃ¡p:**
```bash
# Stop server cÅ©
pkill -f llama-server

# Start láº¡i vá»›i mmproj
cd /home/baobao/Projects/Vintern-1b-v3.5-demo/models/gguf
/home/baobao/Projects/llama.cpp-vintern/build/bin/llama-server \
  -m Vintern-1B-v3_5-Q8_0.gguf \
  --mmproj mmproj-Vintern-1B-v3_5-Q8_0.gguf \
  --host 0.0.0.0 \
  --port 8080
```

### Web UI khÃ´ng load
```bash
# Check port 3000
lsof -i :3000

# Kill vÃ  restart
pkill -f "http.server 3000"
cd /home/baobao/Projects/Vintern-1b-v3.5-demo/web-ui
python3 -m http.server 3000
```

### Inference quÃ¡ cháº­m
- **Hiá»‡n táº¡i:** CPU-only (~2-3s)
- **NÃ¢ng cáº¥p GPU:** Xem [pc-inference-server/README.md](../pc-inference-server/README.md)

## ğŸ“‚ File Structure

```
web-ui/
â”œâ”€â”€ index.html          # Web UI chÃ­nh
â””â”€â”€ README.md          # Docs nÃ y

quick_test.py          # CLI test script
test-fruits.jpg        # Sample image
```

## ğŸ“ Example Prompts

Tiáº¿ng Viá»‡t:
- "MÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y trong áº£nh nÃ y."
- "CÃ³ nhá»¯ng trÃ¡i cÃ¢y nÃ o trong áº£nh?"
- "MÃ u sáº¯c chá»§ Ä‘áº¡o lÃ  gÃ¬?"
- "ÄÃ¢y lÃ  áº£nh chá»¥p á»Ÿ Ä‘Ã¢u?"

English:
- "Describe this image in detail."
- "What fruits do you see?"
- "What are the main colors?"
- "Where was this photo taken?"

## ğŸš€ Next Steps

### Äá»ƒ integrate vÃ o Pi backend:

1. **Copy web UI sang Pi:**
```bash
scp -r web-ui/ pi@<PI_IP>:~/
```

2. **Integrate vÃ o FastAPI:**
```python
# Serve static files
from fastapi.staticfiles import StaticFiles
app.mount("/ui", StaticFiles(directory="web-ui"), name="ui")
```

3. **Or use React:** Copy design vÃ o React frontend hiá»‡n cÃ³

---

**Ready to use!** Má»Ÿ http://localhost:3000 vÃ  test thá»­! ğŸ‰
