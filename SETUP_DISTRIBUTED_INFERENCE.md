# ğŸš€ HÆ¯á»šNG DáºªN SETUP ARCHITECTURE PHÃ‚N TÃN

## ğŸ¯ Má»¥c tiÃªu
**GIáº¢I QUYáº¾T**: Sá»­ dá»¥ng Orange Pi Ä‘á»ƒ cháº¡y VLLM service, Raspberry Pi cháº¡y model inference â†’ KhÃ´ng bá»‹ circular dependency, chia táº£i hiá»‡u quáº£!

## ğŸ“Š Architecture ÄÃºng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER BROWSER                             â”‚
â”‚                  http://192.168.1.14:8000                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ğŸ‡ RASPBERRY PI 4 (192.168.1.14)               â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Backend API (Port 8000)                             â”‚   â”‚
â”‚  â”‚  â”œâ”€ /api/cameras/...        (Camera streaming)      â”‚   â”‚
â”‚  â”‚  â”œâ”€ /api/chat               (User chat) â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â””â”€ /api/generate           (Model inference)   â”‚    â”‚   â”‚
â”‚  â”‚       â””â”€ LocalVinternModel                       â”‚    â”‚   â”‚
â”‚  â”‚          â””â”€ Vintern-1B-v3.5 (PyTorch)           â”‚    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚                                                          â”‚   â”‚
â”‚  ğŸ“¦ Models: /backend/models/Vintern-1B-v3_5             â”‚   â”‚
â”‚  ğŸ’¾ RAM: ~3-4GB cho model inference                     â”‚   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                           â”‚                                   â”‚
                           â”‚ /api/chat gá»i â†’                  â”‚
                           â–¼                                   â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            ğŸŠ ORANGE PI RV2 (192.168.1.16)                  â”‚
â”‚                 RISC-V CPU, No PyTorch                       â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  VLLM Proxy Service (Port 8002)                      â”‚   â”‚
â”‚  â”‚  â””â”€ Proxy Mode                                       â”‚   â”‚
â”‚  â”‚     â””â”€ BACKEND_INFERENCE_URL =                       â”‚   â”‚
â”‚  â”‚        http://192.168.1.14:8000/api/generate        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  âš¡ Vai trÃ²: API Gateway + Request formatting            â”‚ â”‚
â”‚  ğŸ’¾ RAM: ~500MB (khÃ´ng cháº¡y model)                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ Proxy request vá» â†‘
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ Flow Request

### 1. User Chat Request
```
1. Browser â†’ Raspberry Pi: POST /api/chat
   {
     "message": "PhÃ¢n tÃ­ch áº£nh nÃ y",
     "image_base64": "..."
   }

2. Raspberry Pi /api/chat â†’ Orange Pi: POST http://192.168.1.16:8002/analyze
   {
     "messages": [...],
     "max_tokens": 512
   }

3. Orange Pi VLLM Proxy â†’ Raspberry Pi: POST http://192.168.1.14:8000/api/generate
   {
     "messages": [...],
     "temperature": 0.7
   }

4. Raspberry Pi /api/generate:
   - Load Vintern model tá»« /backend/models/
   - Run inference vá»›i PyTorch
   - Return response

5. Orange Pi â†’ Raspberry Pi /api/chat â†’ Browser
   {
     "content": "Trong áº£nh cÃ³...",
     "processing_time": 2.5
   }
```

### 2. Continuous Analysis
```
1. Browser timer (5s) â†’ Raspberry Pi: POST /api/chat (auto)
2. Same flow nhÆ° trÃªn
3. Display káº¿t quáº£ trÃªn UI
```

## âœ… Æ¯u Ä‘iá»ƒm Architecture nÃ y

| Aspect | Benefit |
|--------|---------|
| **KhÃ´ng Loop** | Raspberry Pi `/api/generate` khÃ¡c `/api/chat` â†’ khÃ´ng circular dependency |
| **PhÃ¢n táº£i** | Orange Pi handle API formatting, Raspberry Pi cháº¡y model |
| **Scalable** | CÃ³ thá»ƒ thÃªm nhiá»u Orange Pi proxy cho load balancing |
| **RISC-V Compatible** | Orange Pi khÃ´ng cáº§n PyTorch, chá»‰ cáº§n HTTP proxy |
| **RAM Efficient** | Orange Pi dÃ¹ng Ã­t RAM, Raspberry Pi táº­p trung vÃ o inference |

## ğŸš€ CÃ i Ä‘áº·t

### BÆ°á»›c 1: Download Model (Raspberry Pi)

```bash
cd /home/pi/Projects/Vintern-1b-v3.5-demo
./download_vintern_model.sh
```

**Requirements:**
- ~5GB disk space
- `git-lfs` installed
- HuggingFace account

### BÆ°á»›c 2: Setup ToÃ n bá»™ System

```bash
./setup_local_inference.sh
```

Script nÃ y sáº½:
1. âœ… Kiá»ƒm tra model Ä‘Ã£ download
2. âœ… Cáº¥u hÃ¬nh Raspberry Pi backend (.env)
3. âœ… Cáº¥u hÃ¬nh Orange Pi VLLM proxy (via SSH)
4. âœ… Restart services
5. âœ… Test endpoints

### BÆ°á»›c 3: Verify

```bash
# Check Raspberry Pi backend
curl http://192.168.1.14:8000/api/health

# Check Raspberry Pi inference endpoint
curl http://192.168.1.14:8000/api/model-info

# Check Orange Pi VLLM proxy
curl http://192.168.1.16:8002/

# Test end-to-end (from browser)
# 1. Go to http://192.168.1.14:8000
# 2. Enable "Continuous AI Analysis"
# 3. Watch results appear automatically
```

## ğŸ“ File Changes

### 1. Raspberry Pi Backend

**backend/app/api/inference.py** (NEW)
```python
# Endpoint Ä‘á»ƒ Orange Pi gá»i Ä‘áº¿n
@router.post("/generate")
async def generate_inference(request: GenerateRequest):
    return await local_model.generate_response(...)
```

**backend/app/main.py** (UPDATED)
```python
from app.api.inference import router as inference_router
app.include_router(inference_router, prefix="/api", tags=["inference"])
```

**backend/.env** (UPDATED)
```bash
MODEL_MODE=local
USE_LOCAL_MODEL=true
LOCAL_MODEL_PATH=/home/pi/Projects/Vintern-1b-v3.5-demo/backend/models/Vintern-1B-v3_5
VLLM_SERVICE_URL=http://192.168.1.16:8002
```

### 2. Orange Pi VLLM Service

**vllm-service/.env** (UPDATED)
```bash
USE_PROXY_MODE=true
BACKEND_INFERENCE_URL=http://192.168.1.14:8000/api/generate
MODEL_ID=5CD-AI/Vintern-1B-v3_5
```

## ğŸ§ª Testing

### Manual Test Flow

```bash
# 1. Test Raspberry Pi inference directly
curl -X POST http://192.168.1.14:8000/api/generate \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 50
  }'

# Expected: {"content": "...", "processing_time": 2.5}

# 2. Test Orange Pi proxy
curl -X POST http://192.168.1.16:8002/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [{"role": "user", "content": "Test"}],
    "max_tokens": 50
  }'

# Expected: Same format, proxied from Raspberry Pi

# 3. Test full flow via /api/chat
curl -X POST http://192.168.1.14:8000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "MÃ´ táº£ áº£nh nÃ y",
    "image_base64": "..."
  }'

# Expected: Complete analysis response
```

## ğŸ“Š Resource Usage

| Device | Component | RAM | CPU | Disk |
|--------|-----------|-----|-----|------|
| **Raspberry Pi** | Backend | ~500MB | 20-30% | - |
| | Vintern Model | ~3-4GB | 70-90% (inference) | 5GB |
| | **Total** | **~4-5GB** | **Variable** | **5GB** |
| **Orange Pi** | VLLM Proxy | ~300MB | 5-10% | <100MB |

## ğŸ”¥ Performance

- **First inference**: ~5-10 giÃ¢y (load model)
- **Subsequent inferences**: ~2-3 giÃ¢y
- **Continuous analysis**: Update má»—i 5-30 giÃ¢y (configurable)

## âš ï¸ Troubleshooting

### Issue: Model khÃ´ng load Ä‘Æ°á»£c

```bash
# Check model exists
ls -la /home/pi/Projects/Vintern-1b-v3.5-demo/backend/models/Vintern-1B-v3_5

# Check backend logs
docker logs -f backend

# Expected: "âœ… Model Ä‘Æ°á»£c load thÃ nh cÃ´ng!"
```

### Issue: Orange Pi khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c

```bash
# Test SSH
ssh orangepi@192.168.1.16

# Test VLLM service
ssh orangepi@192.168.1.16 "docker ps | grep vllm"

# Restart VLLM
ssh orangepi@192.168.1.16 "cd ~/Projects/Vintern-1b-v3.5-demo/vllm-service && docker compose restart"
```

### Issue: Circular dependency váº«n cÃ²n

```bash
# Check Orange Pi .env
ssh orangepi@192.168.1.16 "cat ~/Projects/Vintern-1b-v3.5-demo/vllm-service/.env"

# MUST have:
# BACKEND_INFERENCE_URL=http://192.168.1.14:8000/api/generate
# NOT: http://192.168.1.14:8000/api/chat
```

## ğŸ‰ Success Indicators

Khi setup thÃ nh cÃ´ng, báº¡n sáº½ tháº¥y:

1. âœ… Health check shows:
   ```json
   {
     "status": "healthy",
     "model_ready": true,
     "vllm_ready": true,
     "cameras_ready": true
   }
   ```

2. âœ… Browser console shows:
   ```
   âœ“ Continuous Analysis UI is present
   âœ“ Cameras are ready
   âœ“ VLLM is ready
   âœ“ AI Analysis working
   ```

3. âœ… Logs show:
   ```
   [Raspberry Pi] âœ… Model Ä‘Æ°á»£c load thÃ nh cÃ´ng!
   [Orange Pi] âœ… Proxy connected to http://192.168.1.14:8000/api/generate
   [Browser] ğŸ¤– AI phÃ¢n tÃ­ch: Trong áº£nh cÃ³...
   ```

## ğŸ“ Support

Náº¿u gáº·p váº¥n Ä‘á»:
1. Kiá»ƒm tra logs: `docker logs -f backend`
2. Test tá»«ng endpoint riÃªng láº» (xem pháº§n Testing)
3. Verify .env files trÃªn cáº£ 2 devices
4. Restart services: `./setup_local_inference.sh`

---

**TÃ¡c giáº£**: Generated by GitHub Copilot  
**NgÃ y**: 2026-02-14  
**Version**: 1.0
