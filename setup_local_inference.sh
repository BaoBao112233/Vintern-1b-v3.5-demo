#!/bin/bash
# Setup Local Inference trÃªn Raspberry Pi + Orange Pi VLLM Proxy
# Script nÃ y sáº½ cáº¥u hÃ¬nh architecture phÃ¢n tÃ¡n Ä‘Ãºng cÃ¡ch

set -e  # Exit on error

echo "=================================================="
echo "ğŸš€ SETUP RASPBERRY PI + ORANGE PI INFERENCE"
echo "=================================================="
echo ""
echo "Architecture:"
echo "  ğŸ“¹ Raspberry Pi (192.168.1.14:8000)"
echo "     â”œâ”€ Backend API (camera, detection, chat)"
echo "     â””â”€ Inference Engine (/api/generate)"
echo ""
echo "  ğŸ¤– Orange Pi (192.168.1.16:8002)"
echo "     â””â”€ VLLM Proxy â†’ Raspberry Pi /api/generate"
echo ""

# Raspberry Pi IP
RPI_HOST="192.168.1.14"
RPI_BACKEND_PORT="8000"

# Orange Pi IP
ORANGE_PI_HOST="192.168.1.16"
ORANGE_PI_PORT="8002"

# Paths
BACKEND_DIR="/home/pi/Projects/Vintern-1b-v3.5-demo/backend"
ENV_FILE="$BACKEND_DIR/.env"

# ==================================================
# STEP 1: Kiá»ƒm tra model Ä‘Ã£ download chÆ°a
# ==================================================
echo "=================================================="
echo "ğŸ“¦ BÆ¯á»šC 1: KIá»‚M TRA MODEL"
echo "=================================================="
echo ""

MODEL_PATH="$BACKEND_DIR/models/Vintern-1B-v3_5"

if [ ! -d "$MODEL_PATH" ]; then
    echo "âŒ Model chÆ°a Ä‘Æ°á»£c download!"
    echo ""
    read -p "Báº¡n cÃ³ muá»‘n download model ngay bÃ¢y giá»? (y/n): " -n 1 -r
    echo
    if [[ $REPLY =~ ^[Yy]$ ]]; then
        cd /home/pi/Projects/Vintern-1b-v3.5-demo
        ./download_vintern_model.sh
    else
        echo "âš ï¸  Vui lÃ²ng cháº¡y: ./download_vintern_model.sh trÆ°á»›c"
        exit 1
    fi
fi

echo "âœ… Model Ä‘Ã£ tá»“n táº¡i: $MODEL_PATH"
echo ""

# ==================================================
# STEP 2: Config Raspberry Pi Backend
# ==================================================
echo "=================================================="
echo "ğŸ”§ BÆ¯á»šC 2: Cáº¤U HÃŒNH RASPBERRY PI BACKEND"
echo "=================================================="
echo ""

# Backup .env if exists
if [ -f "$ENV_FILE" ]; then
    cp "$ENV_FILE" "$ENV_FILE.backup.$(date +%Y%m%d_%H%M%S)"
    echo "âœ… ÄÃ£ backup .env"
fi

# Update or create .env
cat > "$ENV_FILE" << EOL
# ================================
# Raspberry Pi Backend Configuration
# ================================

# Model Mode - ENABLE LOCAL INFERENCE
MODEL_MODE=local
USE_LOCAL_MODEL=true
LOCAL_MODEL_PATH=$MODEL_PATH

# VLLM Service - Orange Pi
# Backend sáº½ gá»i Orange Pi VLLM, Orange Pi sáº½ proxy vá» /api/generate
VLLM_SERVICE_URL=http://$ORANGE_PI_HOST:$ORANGE_PI_PORT

# Detection Service
DETECTION_SERVICE_URL=http://192.168.1.14:8001
USE_DETECTION_SERVICE=true
MOCK_MODE=false

# Camera Configuration
CAMERA_IDS=cam1,cam2
CAMERA_RTSP_URLS=rtsp://192.168.1.4:554/stream1,rtsp://192.168.1.7:554/stream1

# API Configuration
HOST=0.0.0.0
PORT=$RPI_BACKEND_PORT
LOG_LEVEL=INFO

# HuggingFace (optional, for logging)
HF_TOKEN=your_token_here
EOL

echo "âœ… ÄÃ£ táº¡o $ENV_FILE"
echo ""

# ==================================================
# STEP 3: Config Orange Pi VLLM Proxy
# ==================================================
echo "=================================================="
echo "ğŸ”§ BÆ¯á»šC 3: Cáº¤U HÃŒNH ORANGE PI VLLM PROXY"
echo "=================================================="
echo ""

echo "Äang káº¿t ná»‘i Orange Pi qua SSH..."
echo ""

# Create .env for Orange Pi VLLM service
ORANGE_PI_ENV_CONTENT="# ================================
# Orange Pi VLLM Proxy Configuration
# ================================

# Proxy Mode - Forward to Raspberry Pi Inference
USE_PROXY_MODE=true
BACKEND_INFERENCE_URL=http://$RPI_HOST:$RPI_BACKEND_PORT/api/generate

# Model Info (for display only)
MODEL_ID=5CD-AI/Vintern-1B-v3_5

# Server Configuration
HOST=0.0.0.0
PORT=$ORANGE_PI_PORT
LOG_LEVEL=INFO"

# Try to update Orange Pi via SSH
read -p "Nháº­p máº­t kháº©u Orange Pi (orangepi@$ORANGE_PI_HOST): " -s ORANGE_PASSWORD
echo ""

# Update Orange Pi .env via SSH
echo "$ORANGE_PI_ENV_CONTENT" | ssh orangepi@$ORANGE_PI_HOST "cat > ~/Projects/Vintern-1b-v3.5-demo/vllm-service/.env"

if [ $? -eq 0 ]; then
    echo "âœ… ÄÃ£ cáº­p nháº­t Orange Pi .env"
else
    echo "âŒ KhÃ´ng thá»ƒ cáº­p nháº­t Orange Pi qua SSH"
    echo "Vui lÃ²ng tá»± cáº­p nháº­t file .env trÃªn Orange Pi:"
    echo ""
    echo "$ORANGE_PI_ENV_CONTENT"
    echo ""
    read -p "Nháº¥n Enter khi Ä‘Ã£ cáº­p nháº­t xong..."
fi

echo ""

# ==================================================
# STEP 4: Restart Services
# ==================================================
echo "=================================================="
echo "ğŸ”„ BÆ¯á»šC 4: KHá»I Äá»˜NG Láº I SERVICES"
echo "=================================================="
echo ""

# Restart Raspberry Pi backend (Docker)
echo "ğŸ”„ Khá»Ÿi Ä‘á»™ng láº¡i Raspberry Pi backend..."
cd /home/pi/Projects/Vintern-1b-v3.5-demo
docker compose down
docker compose up -d --build

echo "âœ… Backend Ä‘Ã£ khá»Ÿi Ä‘á»™ng"
echo ""

# Restart Orange Pi VLLM service
echo "ğŸ”„ Khá»Ÿi Ä‘á»™ng láº¡i Orange Pi VLLM service..."
ssh orangepi@$ORANGE_PI_HOST "cd ~/Projects/Vintern-1b-v3.5-demo/vllm-service && docker compose down && docker compose up -d"

if [ $? -eq 0 ]; then
    echo "âœ… Orange Pi VLLM service Ä‘Ã£ khá»Ÿi Ä‘á»™ng"
else
    echo "âš ï¸  KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng Orange Pi service tá»± Ä‘á»™ng"
    echo "Vui lÃ²ng cháº¡y trÃªn Orange Pi:"
    echo "  cd ~/Projects/Vintern-1b-v3.5-demo/vllm-service"
    echo "  docker compose up -d"
fi

echo ""

# ==================================================
# STEP 5: Test Services
# ==================================================
echo "=================================================="
echo "ğŸ§ª BÆ¯á»šC 5: KIá»‚M TRA SERVICES"
echo "=================================================="
echo ""

# Wait for services to start
echo "Äá»£i services khá»Ÿi Ä‘á»™ng (30 giÃ¢y)..."
sleep 30

# Test Raspberry Pi backend
echo "ğŸ” Test Raspberry Pi backend..."
BACKEND_HEALTH=$(curl -s http://$RPI_HOST:$RPI_BACKEND_PORT/api/health)
echo "Response: $BACKEND_HEALTH"
echo ""

# Test Raspberry Pi inference endpoint
echo "ğŸ” Test Raspberry Pi inference endpoint..."
INFERENCE_TEST=$(curl -s -X POST http://$RPI_HOST:$RPI_BACKEND_PORT/api/model-info)
echo "Response: $INFERENCE_TEST"
echo ""

# Test Orange Pi VLLM
echo "ğŸ” Test Orange Pi VLLM..."
VLLM_HEALTH=$(curl -s http://$ORANGE_PI_HOST:$ORANGE_PI_PORT/)
echo "Response: $VLLM_HEALTH"
echo ""

# ==================================================
# DONE!
# ==================================================
echo "=================================================="
echo "âœ… HOÃ€N Táº¤T SETUP!"
echo "=================================================="
echo ""
echo "ğŸ“Š Services:"
echo "  âœ… Raspberry Pi Backend:    http://$RPI_HOST:$RPI_BACKEND_PORT"
echo "  âœ… Raspberry Pi Inference:  http://$RPI_HOST:$RPI_BACKEND_PORT/api/generate"
echo "  âœ… Orange Pi VLLM Proxy:    http://$ORANGE_PI_HOST:$ORANGE_PI_PORT"
echo ""
echo "ğŸŒ Web UI:"
echo "  ğŸ‘‰ http://$RPI_HOST:$RPI_BACKEND_PORT/"
echo ""
echo "ğŸ“ Test AI Analysis:"
echo "  1. Má»Ÿ web UI"
echo "  2. Báº­t 'Continuous AI Analysis'"
echo "  3. Xem káº¿t quáº£ phÃ¢n tÃ­ch tá»± Ä‘á»™ng"
echo ""
echo "ğŸ” Kiá»ƒm tra logs:"
echo "  docker logs -f backend        # Raspberry Pi"
echo "  ssh orangepi@$ORANGE_PI_HOST 'docker logs -f vllm-service'"
echo ""
