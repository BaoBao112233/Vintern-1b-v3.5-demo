# Client Library - Raspberry Pi â†’ PC Communication

Client code Ä‘á»ƒ Raspberry Pi giao tiáº¿p vá»›i PC inference server qua LAN.

## ğŸ“‹ Prerequisites

### TrÃªn PC
1. âœ… PC inference server Ä‘ang cháº¡y
2. âœ… Port 8080 má»Ÿ (khÃ´ng bá»‹ firewall block)
3. âœ… PC cÃ³ static IP hoáº·c ghi nhá»› IP hiá»‡n táº¡i

### TrÃªn Raspberry Pi
1. Python 3.8+
2. Installed packages:
   ```bash
   pip install requests pillow
   ```

## ğŸŒ Network Setup

### 1. Láº¥y PC IP Address

**TrÃªn PC cháº¡y:**
```bash
ip addr show | grep "inet " | grep -v "127.0.0.1"
# hoáº·c
hostname -I
```

VÃ­ dá»¥ output: `192.168.1.100`

### 2. Set Static IP (Khuyáº¿n nghá»‹)

**Option A: Qua Router**
- VÃ o router admin panel
- DHCP settings â†’ Reserve IP cho MAC address cá»§a PC
- VÃ­ dá»¥: Reserve `192.168.1.100` cho PC

**Option B: TrÃªn PC (Ubuntu)**
```bash
# Edit netplan config
sudo nano /etc/netplan/01-netcfg.yaml

# ThÃªm config:
network:
  version: 2
  ethernets:
    enp3s0:  # Thay báº±ng interface thá»±c táº¿ (xem vá»›i 'ip a')
      addresses:
        - 192.168.1.100/24
      gateway4: 192.168.1.1
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]

# Apply changes
sudo netplan apply
```

### 3. Má»Ÿ Firewall Port

**Option A: UFW (Ubuntu)**
```bash
# Check firewall status
sudo ufw status

# Allow port 8080
sudo ufw allow 8080/tcp

# Verify
sudo ufw status
```

**Option B: iptables**
```bash
# Add rule
sudo iptables -A INPUT -p tcp --dport 8080 -j ACCEPT

# Save rules (Ubuntu)
sudo netfilter-persistent save

# List rules
sudo iptables -L -n | grep 8080
```

**Option C: Disable firewall (not recommended)**
```bash
sudo ufw disable
```

## ğŸ§ª Test Connection

### TrÃªn Raspberry Pi:

**1. Test basic connectivity**
```bash
cd /home/baobao/Projects/Vintern-1b-v3.5-demo/client

# Sá»­a PC_IP trong script hoáº·c pass as argument
python test_connection.py 192.168.1.100
```

Expected output:
```
âœ… Network Ping - PASS
âœ… Port Check - PASS  
âœ… HTTP Health - PASS
âœ… Bandwidth Test - PASS
```

**2. Test inference vá»›i client**
```bash
# Download client files to Pi (hoáº·c copy qua)
scp pc_inference_client.py pi@<PI_IP>:~/

# TrÃªn Pi:
python pc_inference_client.py test-image.jpg "MÃ´ táº£ áº£nh nÃ y"
```

## ğŸ“š Usage Examples

### Example 1: Basic Usage

```python
from pc_inference_client import PCInferenceClient

# Initialize client vá»›i PC IP
client = PCInferenceClient(pc_host="192.168.1.100")

# Health check
if client.health_check():
    print("PC is ready!")
else:
    print("PC is not available")
    exit(1)

# Inference
result = client.chat_completion(
    image="path/to/image.jpg",
    prompt="MÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y"
)

if "error" not in result:
    print(f"Response: {result['content']}")
    print(f"Time: {result['elapsed_time']:.2f}s")
else:
    print(f"Error: {result['error']}")

client.close()
```

### Example 2: Vá»›i Detection Results

```python
from pc_inference_client import PCInferenceClient

client = PCInferenceClient(pc_host="192.168.1.100")

# Giáº£ sá»­ báº¡n Ä‘Ã£ detect objects vá»›i YOLO
detections = [
    {"class": "person", "confidence": 0.95, "bbox": [100, 200, 150, 300]},
    {"class": "car", "confidence": 0.88, "bbox": [300, 150, 200, 250]}
]

# VLM sáº½ analyze dá»±a trÃªn detected objects
result = client.analyze_detections(
    image="frame.jpg",
    detections=detections
)

print(result['content'])
client.close()
```

### Example 3: Stream Processing (Camera)

```python
import cv2
from pc_inference_client import PCInferenceClient
from PIL import Image
import numpy as np

client = PCInferenceClient(pc_host="192.168.1.100")

# RTSP stream
cap = cv2.VideoCapture("rtsp://admin:pass@192.168.1.4/cam/realmonitor?channel=1&subtype=1")

frame_count = 0
while True:
    ret, frame = cap.read()
    if not ret:
        break
    
    frame_count += 1
    
    # Process every 30 frames (1 frame per second at 30fps)
    if frame_count % 30 == 0:
        # Convert BGR to RGB
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(rgb_frame)
        
        # Inference
        result = client.chat_completion(
            image=pil_image,
            prompt="CÃ³ gÃ¬ báº¥t thÆ°á»ng khÃ´ng?"
        )
        
        if "error" not in result:
            print(f"[Frame {frame_count}] {result['content']}")
        else:
            print(f"[Frame {frame_count}] Error: {result['error']}")
    
    # Exit on 'q'
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
client.close()
```

### Example 4: Async Multiple Cameras

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor
from pc_inference_client import PCInferenceClient

def process_camera(camera_id, pc_host):
    """Process one camera in separate thread"""
    client = PCInferenceClient(pc_host=pc_host)
    
    # Capture frame from camera
    frame_path = f"camera_{camera_id}_frame.jpg"
    
    result = client.chat_completion(
        image=frame_path,
        prompt=f"Camera {camera_id}: MÃ´ táº£ tÃ¬nh hÃ¬nh"
    )
    
    client.close()
    return camera_id, result

async def main():
    pc_host = "192.168.1.100"
    camera_ids = [1, 2]
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        loop = asyncio.get_event_loop()
        
        tasks = [
            loop.run_in_executor(executor, process_camera, cam_id, pc_host)
            for cam_id in camera_ids
        ]
        
        results = await asyncio.gather(*tasks)
        
        for cam_id, result in results:
            print(f"\nCamera {cam_id}:")
            if "error" not in result:
                print(result['content'])
            else:
                print(f"Error: {result['error']}")

asyncio.run(main())
```

## ğŸ”§ Configuration

### Client Settings

```python
client = PCInferenceClient(
    pc_host="192.168.1.100",  # PC IP address
    pc_port=8080,              # Server port
    timeout=30,                # Request timeout (seconds)
    max_retries=3,             # Retry failed requests
    retry_delay=1.0            # Delay between retries
)
```

### Image Encoding Options

```python
result = client.chat_completion(
    image="image.jpg",
    prompt="...",
    max_tokens=200,           # Max tokens in response
    temperature=0.1           # 0=deterministic, 1=creative
)

# Hoáº·c vá»›i custom encoding
image_url = client.encode_image(
    image="large_image.jpg",
    max_size=(1024, 1024),   # Resize to max 1024x1024
    quality=85                # JPEG quality
)
```

## ğŸš¨ Troubleshooting

### Pi khÃ´ng connect Ä‘Æ°á»£c PC

**1. Check network:**
```bash
ping 192.168.1.100
```

**2. Check port:**
```bash
nc -zv 192.168.1.100 8080
# hoáº·c
telnet 192.168.1.100 8080
```

**3. Check PC server:**
TrÃªn PC:
```bash
# Server cÃ³ Ä‘ang cháº¡y?
ps aux | grep llama-server

# Port cÃ³ listening?
netstat -tlnp | grep 8080

# Firewall status
sudo ufw status
```

**4. Check firewall rules:**
```bash
# TrÃªn PC
sudo iptables -L -n | grep 8080
```

### Request timeout

**NguyÃªn nhÃ¢n:**
- PC CPU-only inference cháº­m (~2-3s)
- Network latency cao
- PC overloaded

**Giáº£i phÃ¡p:**
```python
# TÄƒng timeout
client = PCInferenceClient(
    pc_host="192.168.1.100",
    timeout=60  # 60 seconds
)

# Hoáº·c upgrade PC lÃªn GPU
# â†’ Xem pc-inference-server/README.md
```

### Connection refused

**NguyÃªn nhÃ¢n:**
- Server khÃ´ng cháº¡y
- Wrong IP/port
- Firewall block

**Debug:**
```bash
# TrÃªn PC
curl http://localhost:8080/health  # OK?
curl http://192.168.1.100:8080/health  # OK?

# TrÃªn Pi
curl http://192.168.1.100:8080/health  # Fail?
```

## ğŸ“Š Performance Tips

### 1. Resize images trÆ°á»›c khi gá»­i
```python
# Good - resize to 1024x1024
result = client.chat_completion(
    image="large_4k_image.jpg",  # Will auto-resize
    prompt="..."
)
```

### 2. Batch processing thay vÃ¬ realtime
```python
# Collect frames
frames = []
for i in range(10):
    frame = capture_frame()
    frames.append(frame)

# Process batch
for frame in frames:
    result = client.chat_completion(frame, "...")
```

### 3. Rate limiting
```python
import time

last_inference = 0
MIN_INTERVAL = 1.0  # 1 second between inferences

while True:
    frame = capture_frame()
    
    now = time.time()
    if now - last_inference >= MIN_INTERVAL:
        result = client.chat_completion(frame, "...")
        last_inference = now
```

## ğŸ“¡ Network Performance

### Bandwidth Usage Estimate

| Image Size | Encoded Size | Per Request | 1 FPS | 5 FPS |
|------------|--------------|-------------|-------|-------|
| 640x480 | ~50 KB | ~70 KB | 0.56 Mbps | 2.8 Mbps |
| 1024x768 | ~100 KB | ~140 KB | 1.1 Mbps | 5.6 Mbps |
| 1920x1080 | ~200 KB | ~280 KB | 2.2 Mbps | 11.2 Mbps |

**Khuyáº¿n nghá»‹:**
- 100Mbps LAN: âœ… OK cho 2 cameras @ 5 FPS
- 1Gbps LAN: âœ… OK cho nhiá»u cameras

### Latency Estimate

```
Pi â†’ PC round trip:
â”œâ”€ Network: ~1-5ms (LAN)
â”œâ”€ Encoding: ~50-100ms (Pi resize/encode)
â”œâ”€ Inference: ~2000-3000ms (PC CPU-only)
â”œâ”€ Decoding: ~10ms
â””â”€ Total: ~2-3 seconds

Vá»›i PC GPU:
â””â”€ Total: ~300-500ms (nhanh hÆ¡n 5-10x)
```

## ğŸ”— Integration vá»›i Backend

Xem [backend integration examples](../backend/README.md) Ä‘á»ƒ integrate client vÃ o FastAPI backend.

---

**Ready to use!** Cháº¡y `test_connection.py` Ä‘á»ƒ báº¯t Ä‘áº§u.
