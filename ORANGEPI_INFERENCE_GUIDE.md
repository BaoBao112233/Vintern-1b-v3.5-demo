# Ch·∫°y Vintern-1B tr√™n Orange Pi RISC-V

## üéØ M·ª•c Ti√™u

Ch·∫°y model Vintern-1B **native** tr√™n Orange Pi RV2 (RISC-V) ƒë·ªÉ ph√¢n t√°n t·∫£i, gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ:
- ‚úÖ Kh√¥ng l√†m qu√° t·∫£i Raspberry Pi 4
- ‚úÖ S·ª≠ d·ª•ng computing power c·ªßa Orange Pi
- ‚úÖ Lo·∫°i b·ªè circular dependency c·ªßa proxy mode
- ‚úÖ AI inference ch·∫°y song song v·ªõi camera streaming

## üîß Gi·∫£i Ph√°p: llama.cpp

Orange Pi RV2 c√≥ CPU RISC-V kh√¥ng h·ªó tr·ª£ PyTorch, nh∆∞ng c√≥ th·ªÉ ch·∫°y **llama.cpp**:

### T·∫°i sao llama.cpp?
- ‚úÖ **Pure C++**: Compile ƒë∆∞·ª£c tr√™n m·ªçi ki·∫øn tr√∫c (ARM, RISC-V, x86)
- ‚úÖ **Hi·ªáu qu·∫£ cao**: T·ªëi ∆∞u cho CPU inference
- ‚úÖ **GGUF format**: Model ƒë∆∞·ª£c quantize, ti·∫øt ki·ªám RAM
- ‚úÖ **Vision support**: H·ªó tr·ª£ multimodal (text + image)
- ‚úÖ **Active development**: C·ªông ƒë·ªìng l·ªõn, update th∆∞·ªùng xuy√™n

### Quantization
Model Vintern-1B (1.3GB) s·∫Ω ƒë∆∞·ª£c convert sang **GGUF Q8_0**:
- Original FP16: ~2.6GB
- Q8_0: ~1.4GB (8-bit quantization)
- Q4_K_M: ~800MB (4-bit, n·∫øu c·∫ßn nh·∫π h∆°n)
- Minimal loss trong accuracy v·ªõi Q8_0

## üì¶ Ki·∫øn Tr√∫c M·ªõi

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Raspberry Pi 4 (ARM64)                ‚îÇ
‚îÇ         192.168.1.14:8000                       ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  ‚Ä¢ FastAPI Backend                              ‚îÇ
‚îÇ  ‚Ä¢ Camera streaming (2 cameras)                 ‚îÇ
‚îÇ  ‚Ä¢ Object detection (Coral USB)                 ‚îÇ
‚îÇ  ‚Ä¢ Web UI (HTML/JS)                             ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚îÇ HTTP Request
                 ‚îÇ POST /analyze (image + prompt)
                 ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          Orange Pi RV2 (RISC-V)                 ‚îÇ
‚îÇ         192.168.1.16:8003                       ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  Port 8003: FastAPI Wrapper                     ‚îÇ
‚îÇ    ‚îî‚îÄ> /analyze ‚Üí Format request                ‚îÇ
‚îÇ    ‚îî‚îÄ> /chat    ‚Üí Forward to llama-server       ‚îÇ
‚îÇ    ‚îî‚îÄ> /health  ‚Üí Status check                  ‚îÇ
‚îÇ                                                 ‚îÇ
‚îÇ  Port 8002: llama-server                        ‚îÇ
‚îÇ    ‚îî‚îÄ> Vintern-1B GGUF Q8_0                     ‚îÇ
‚îÇ    ‚îî‚îÄ> Native RISC-V inference                  ‚îÇ
‚îÇ    ‚îî‚îÄ> 2048 context, 256 max tokens             ‚îÇ
‚îÇ                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Deployment

### Option 1: Automatic (Khuy·∫øn ngh·ªã)

Ch·∫°y t·ª´ Raspberry Pi, script s·∫Ω t·ª± ƒë·ªông SSH v√† setup Orange Pi:

```bash
cd /home/pi/Projects/Vintern-1b-v3.5-demo
./deploy_orangepi_inference.sh
```

Script n√†y s·∫Ω:
1. Copy `setup_orangepi_llamacpp.sh` sang Orange Pi
2. SSH v√†o Orange Pi v√† ch·∫°y setup (build llama.cpp + convert model)
3. T·∫°o systemd services tr√™n Orange Pi
4. Start services
5. Test API
6. Update backend/.env tr√™n Raspberry Pi
7. Restart backend Docker container

**Th·ªùi gian**: 20-30 ph√∫t (build llama.cpp + convert model)

### Option 2: Manual

#### Tr√™n Orange Pi (192.168.1.16):

```bash
# Copy script t·ª´ Raspberry Pi
scp pi@192.168.1.14:~/Projects/Vintern-1b-v3.5-demo/setup_orangepi_llamacpp.sh ~/

# Run setup
bash ~/setup_orangepi_llamacpp.sh

# Start services
sudo systemctl enable vintern-llamacpp
sudo systemctl start vintern-llamacpp
sudo systemctl enable vintern-wrapper
sudo systemctl start vintern-wrapper

# Check status
sudo systemctl status vintern-llamacpp
sudo systemctl status vintern-wrapper

# Test API
curl http://localhost:8003/health
curl http://localhost:8003/model/info
```

#### Tr√™n Raspberry Pi (192.168.1.14):

```bash
# Update backend/.env
echo "VLLM_SERVICE_URL=http://192.168.1.16:8003" >> backend/.env

# Restart backend
docker-compose restart backend

# Test
curl http://localhost:8000/api/health
```

## üìä Services tr√™n Orange Pi

### 1. vintern-llamacpp.service
- **Port**: 8002
- **Command**: `llama-server --model ~/models/vintern-1b-gguf/vintern-1b-q8_0.gguf`
- **Purpose**: Native inference engine
- **Logs**: `sudo journalctl -u vintern-llamacpp -f`

### 2. vintern-wrapper.service
- **Port**: 8003
- **Command**: `python3 llamacpp_wrapper.py`
- **Purpose**: API compatibility layer (FastAPI)
- **Logs**: `sudo journalctl -u vintern-wrapper -f`

### API Endpoints (port 8003):

```bash
# Health check
GET /health
‚Üí {"status": "healthy", "model": "Vintern-1B-v3_5", "backend": "llama.cpp"}

# Model info
GET /model/info
‚Üí {"model_id": "5CD-AI/Vintern-1B-v3_5", "mode": "native", "format": "GGUF Q8_0"}

# Analyze image
POST /analyze
{
  "image": "base64_encoded_image",
  "prompt": "Describe this image",
  "max_tokens": 256,
  "temperature": 0.7
}

# Chat (with optional image)
POST /chat
{
  "message": "What do you see?",
  "image": "base64_encoded_image",  # Optional
  "max_tokens": 256
}
```

## üß™ Testing

### Test Orange Pi API tr·ª±c ti·∫øp:

```bash
# From Raspberry Pi
curl http://192.168.1.16:8003/health

# Health check with details
curl -s http://192.168.1.16:8003/health | jq .

# Model info
curl -s http://192.168.1.16:8003/model/info | jq .
```

### Test v·ªõi image:

```bash
# Capture frame from camera
curl -s http://localhost:8000/api/cameras/1/frame | jq -r '.image_base64' > /tmp/frame.b64

# Send to Orange Pi for analysis
curl -X POST http://192.168.1.16:8003/analyze \
  -H "Content-Type: application/json" \
  -d "{\"image\": \"$(cat /tmp/frame.b64)\", \"prompt\": \"M√¥ t·∫£ chi ti·∫øt nh·ªØng g√¨ b·∫°n th·∫•y trong h√¨nh\"}"
```

### Test t·ª´ Web UI:

1. M·ªü http://192.168.1.14:8000/
2. Ki·ªÉm tra VLLM status trong status bar (ph·∫£i l√† "Ready")
3. Click "Analyze with AI" tr√™n camera 1 ho·∫∑c 2
4. Ho·∫∑c enable "Continuous AI Analysis"
5. Xem k·∫øt qu·∫£ trong AI analysis box

## üìà Performance

### Orange Pi RV2 Specs:
- **CPU**: Ky X1 (RISC-V), 2 cores @ 1.5GHz
- **RAM**: 2GB ho·∫∑c 4GB
- **Expected inference**: 2-5 tokens/second (text), 10-20s per image

### Model Size:
- **Original**: ~2.6GB (FP16)
- **GGUF Q8_0**: ~1.4GB
- **GGUF Q4_K_M**: ~800MB (optional, faster but less accurate)

### Resource Usage:
- **RAM**: ~1.5GB cho model + ~500MB cho runtime
- **CPU**: ~80-100% during inference
- **Idle**: ~2-5% CPU

## üõ†Ô∏è Troubleshooting

### Service kh√¥ng start

```bash
# Check logs
sudo journalctl -u vintern-llamacpp -n 50
sudo journalctl -u vintern-wrapper -n 50

# Check if llama-server is running
ps aux | grep llama-server

# Check port
sudo netstat -tlnp | grep :8002
sudo netstat -tlnp | grep :8003
```

### Model kh√¥ng load

```bash
# Check model file exists
ls -lh ~/models/vintern-1b-gguf/vintern-1b-q8_0.gguf

# Check RAM
free -h

# Try loading manually
~/llama.cpp/llama-server \
  --model ~/models/vintern-1b-gguf/vintern-1b-q8_0.gguf \
  --host 127.0.0.1 \
  --port 8002 \
  --ctx-size 2048 \
  --threads 2
```

### Build llama.cpp failed

```bash
# Install build dependencies
sudo apt update
sudo apt install -y build-essential git cmake

# Clean and rebuild
cd ~/llama.cpp
make clean
make -j2  # Use 2 threads to avoid OOM
```

### API kh√¥ng response

```bash
# Check if wrapper can reach llama-server
curl http://localhost:8002/health

# Check Python wrapper
ps aux | grep llamacpp_wrapper

# Restart services
sudo systemctl restart vintern-llamacpp
sleep 5
sudo systemctl restart vintern-wrapper
```

### Backend kh√¥ng connect ƒë∆∞·ª£c Orange Pi

```bash
# From Raspberry Pi, test connectivity
ping 192.168.1.16
curl http://192.168.1.16:8003/health

# Check backend .env
cat backend/.env | grep VLLM_SERVICE_URL

# Should be: VLLM_SERVICE_URL=http://192.168.1.16:8003
```

## üîÑ Update Model

ƒê·ªÉ update model ho·∫∑c th·ª≠ quantization kh√°c:

```bash
# On Orange Pi
cd ~/llama.cpp

# List available quantizations
./llama-quantize

# Convert to Q4_K_M (faster, smaller, nh∆∞ng k√©m accuracy h∆°n)
./llama-quantize \
  ~/models/vintern-1b-gguf/vintern-1b-f16.gguf \
  ~/models/vintern-1b-gguf/vintern-1b-q4_k_m.gguf \
  Q4_K_M

# Update service to use new model
sudo systemctl edit vintern-llamacpp

# Change --model path, save, then:
sudo systemctl daemon-reload
sudo systemctl restart vintern-llamacpp
```

## üìö References

- llama.cpp: https://github.com/ggerganov/llama.cpp
- GGUF format: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- Quantization guide: https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/README.md
- Vision model support: https://github.com/ggerganov/llama.cpp/blob/master/examples/llava/README.md

## ‚úÖ Checklist

Sau khi deploy, verify:

- [ ] Orange Pi: `systemctl status vintern-llamacpp` ‚Üí active (running)
- [ ] Orange Pi: `systemctl status vintern-wrapper` ‚Üí active (running)
- [ ] Orange Pi: `curl http://localhost:8003/health` ‚Üí {"status": "healthy"}
- [ ] Raspberry Pi: `cat backend/.env | grep VLLM` ‚Üí http://192.168.1.16:8003
- [ ] Raspberry Pi: `docker ps | grep backend` ‚Üí Up
- [ ] Web UI: http://192.168.1.14:8000/ ‚Üí cameras streaming
- [ ] Web UI: Status bar ‚Üí "VLLM: Ready"
- [ ] Web UI: Click "Analyze with AI" ‚Üí Nh·∫≠n ƒë∆∞·ª£c response t·ª´ Orange Pi

N·∫øu t·∫•t c·∫£ ‚úÖ, h·ªá th·ªëng ƒë√£ s·∫µn s√†ng!
