# VLLM Service Configuration

# Service Port
PORT=8002
HOST=0.0.0.0

# Network Configuration
SERVICE_IP=192.168.100.20
DETECTION_SERVICE_URL=http://192.168.100.10:8001

# Model Configuration
<<<<<<< HEAD
MODEL_ID=5CD-AI/Vintern-1B-v3.5
=======
MODEL_ID=5CD-AI/Vintern-1B-v3_5

# Backend Inference Service
# IMPORTANT: This service runs on RISC-V which cannot run PyTorch
# Point this to a backend service with GPU that can run the actual model
BACKEND_INFERENCE_URL=http://192.168.1.14:8000

# Legacy settings (not used in proxy mode)
>>>>>>> 343ee07b5a6535a225b421480837bfeacfbdc1d3
HUGGINGFACE_TOKEN=your_hf_token_here
MODEL_PATH=models/Vintern-1B-v3_5
USE_QUANTIZATION=true
QUANTIZATION_BITS=8

# Generation Settings
MAX_NEW_TOKENS=256
TEMPERATURE=0.7
TOP_P=0.9

# Memory Settings
LOW_CPU_MEM_USAGE=true
DEVICE_MAP=auto

# Logging
LOG_LEVEL=INFO
