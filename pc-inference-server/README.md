# PC Inference Server - Vintern 1B

Server inference cháº¡y trÃªn PC (GTX 1050Ti) Ä‘á»ƒ Raspberry Pi gá»­i request qua LAN.

## ğŸ“Š Cáº¥u HÃ¬nh Hiá»‡n Táº¡i

### Hardware
- **CPU**: Intel Core i3-10105F @ 3.70GHz (4C/8T)
- **RAM**: 16GB
- **GPU**: GTX 1050 Ti 4GB VRAM
- **Driver**: NVIDIA 580.126.09
- **CUDA**: 13.0 (driver-only)

### Software
- **Model**: Vintern-1B-v3.5 GGUF Q8_0 (~1GB total)
  - LLM: 644MB
  - Vision Encoder: 318MB
- **Engine**: llama.cpp (build 8067)
- **Mode**: **CPU-only** (chÆ°a cÃ³ CUDA Toolkit)
- **API**: OpenAI-compatible endpoints

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### 1. Khá»Ÿi Ä‘á»™ng server

```bash
cd /home/baobao/Projects/Vintern-1b-v3.5-demo/pc-inference-server
chmod +x start_server.sh
./start_server.sh
```

Server sáº½:
- Listen trÃªn `0.0.0.0:8080` (táº¥t cáº£ interfaces)  
- Cho phÃ©p Raspberry Pi connect qua LAN
- Log output vÃ o `logs/server_*.log`

### 2. Test server

```bash
# Test health endpoint
curl http://localhost:8080/health

# Test inference vá»›i Python script
chmod +x test_server.py
python3 test_server.py path/to/image.jpg "MÃ´ táº£ áº£nh nÃ y"
```

### 3. Dá»«ng server

```bash
chmod +x stop_server.sh
./stop_server.sh
```

## ğŸ“¡ API Endpoints

### Health Check
```bash
GET http://<PC_IP>:8080/health
```

Response:
```json
{"status":"ok"}
```

### Chat Completions (OpenAI-compatible)
```bash
POST http://<PC_IP>:8080/v1/chat/completions
Content-Type: application/json
```

Request body:
```json
{
  "messages": [
    {
      "role": "user",
      "content": [
        {
          "type": "image_url",
          "image_url": {
            "url": "data:image/jpeg;base64,<BASE64_IMAGE>"
          }
        },
        {
          "type": "text",
          "text": "MÃ´ táº£ áº£nh nÃ y"
        }
      ]
    }
  ],
  "max_tokens": 200,
  "temperature": 0.1
}
```

## ğŸ”§ Tá»‘i Æ¯u Performance

### Hiá»‡n táº¡i: CPU-only
- âœ… ÄÆ¡n giáº£n, stable
- âš ï¸ Cháº­m hÆ¡n GPU (~2-3s per inference)

### Upgrade lÃªn GPU (náº¿u cáº§n):

1. **CÃ i CUDA Toolkit**:
```bash
sudo apt install nvidia-cuda-toolkit
```

2. **Rebuild llama.cpp vá»›i CUDA**:
```bash
cd /home/baobao/Projects/llama.cpp-vintern
rm -rf build
cmake -B build -DGGML_CUDA=ON -DCMAKE_CUDA_ARCHITECTURES=61
cmake --build build --config Release --target llama-server -j4
```

3. **Edit `start_server.sh`**: ThÃªm flag `-ngl 99` Ä‘á»ƒ offload layers lÃªn GPU

4. **Restart server**

Performance vá»›i GPU:
- âœ… Nhanh hÆ¡n 5-10x (~0.3-0.5s per inference)
- âœ… Fit vÃ o 4GB VRAM (model chá»‰ ~1GB)
- âœ… Giáº£m CPU usage

## ğŸŒ Network Configuration

### Static IP (khuyáº¿n nghá»‹)
Äá»ƒ Pi dá»… dÃ ng connect, nÃªn set static IP cho PC:

```bash
# Check current IP
ip addr show

# Example: PC = 192.168.1.100, Pi will use this IP
```

### Firewall  
Má»Ÿ port 8080 náº¿u firewall Ä‘ang báº­t:
```bash
sudo ufw allow 8080/tcp
# hoáº·c
sudo iptables -A INPUT -p tcp --dport 8080 -j ACCEPT
```

### Test tá»« Pi
```bash
# Tá»« Raspberry Pi
curl http://<PC_IP>:8080/health
```

## ğŸ“Š Monitoring

### Check GPU usage (náº¿u cÃ³ CUDA):
```bash
watch -n 1 nvidia-smi
```

### Check CPU usage:
```bash
htop
```

### Check server logs:
```bash
tail -f logs/server_*.log
```

## ğŸ”¥ Troubleshooting

### Server khÃ´ng start
- Kiá»ƒm tra llama-server binary tá»“n táº¡i
- Kiá»ƒm tra model files Ä‘Ã£ download
- Check port 8080 cÃ³ bá»‹ chiáº¿m khÃ´ng: `lsof -i :8080`

### Pi khÃ´ng connect Ä‘Æ°á»£c
- Ping tá»« Pi sang PC: `ping <PC_IP>`
- Check firewall PC
- Verify server Ä‘ang listen: `netstat -tlnp | grep 8080`

### Inference quÃ¡ cháº­m
- CPU-only: bÃ¬nh thÆ°á»ng, chá» 2-5s
- Cáº§n nhanh hÆ¡n â†’ rebuild vá»›i CUDA (xem pháº§n Upgrade)

### Out of memory
- Model 1GB fit OK trong 4GB VRAM
- Náº¿u crash â†’ giáº£m `ctx_size` trong `start_server.sh`  
- Hoáº·c dÃ¹ng Q4 quantization (nháº¹ hÆ¡n)

## ğŸ“ Next Steps

1. âœ… **Server Ä‘ang cháº¡y** - test thá»­ inference
2. â­ï¸ **Setup Raspberry Pi** - cáº¥u hÃ¬nh Pi gá»­i request qua LAN
3. â­ï¸ **Integrate camera RTSP** - Pi nháº­n camera feed
4. â­ï¸ **Full pipeline** - Camera â†’ Detection â†’ VLM inference â†’ Response

## ğŸ¯ Performance Benchmark

| Config | Inference Time | GPU VRAM | CPU Usage |
|--------|---------------|----------|-----------|
| CPU-only (current) | ~2-3s | 0 MB | ~100% (4 cores) |
| GPU CUDA (todo) | ~0.3-0.5s | ~1.2 GB | ~20% |

---

**LiÃªn há»‡**: Äang test, sáº½ update khi cÃ³ váº¥n Ä‘á»!
