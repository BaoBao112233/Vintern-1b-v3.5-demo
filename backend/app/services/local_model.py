"""
Local Model Service cho Vintern-1B-v3.5
Ch·∫°y model tr·ª±c ti·∫øp tr√™n m√°y thay v√¨ qua HuggingFace API
"""

import os
import json
import torch
import asyncio
from pathlib import Path
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import logging

logger = logging.getLogger(__name__)

class LocalVinternModel:
    def __init__(self, model_path: str = None):
        self.model_path = model_path or self._get_model_path()
        self.tokenizer: Optional[AutoTokenizer] = None
        self.model: Optional[AutoModelForCausalLM] = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.is_loaded = False
        
    def _get_model_path(self) -> str:
        """T√¨m model path t·ª´ config ho·∫∑c default location"""
        # Th·ª≠ t√¨m trong th∆∞ m·ª•c models v·ªõi t√™n git clone
        git_clone_path = Path(__file__).parent.parent.parent / "models" / "Vintern-1B-v3_5"
        if git_clone_path.exists():
            return str(git_clone_path)
            
        # Th·ª≠ t√¨m trong th∆∞ m·ª•c models v·ªõi t√™n c≈©
        old_path = Path(__file__).parent.parent.parent / "models" / "vintern-1b-v3.5"
        if old_path.exists():
            return str(old_path)
        
        # N·∫øu kh√¥ng c√≥, th·ª≠ t·ª´ environment
        env_path = os.getenv("LOCAL_MODEL_PATH")
        if env_path:
            return env_path
            
        # Default fallback
        return str(git_clone_path)
    
    async def initialize(self) -> bool:
        """Load model v√† tokenizer"""
        try:
            logger.info(f"üöÄ ƒêang load local model t·ª´: {self.model_path}")
            logger.info(f"üîß Device: {self.device}")
            
            # Ki·ªÉm tra model path t·ªìn t·∫°i
            if not Path(self.model_path).exists():
                raise FileNotFoundError(f"Model path kh√¥ng t·ªìn t·∫°i: {self.model_path}")
            
            # Load tokenizer
            logger.info("üì• ƒêang load tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # Load model
            logger.info("üì• ƒêang load model...")
            torch_dtype = torch.float16 if self.device == "cuda" else torch.float32
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch_dtype,
                device_map="auto" if self.device == "cuda" else None,
                low_cpu_mem_usage=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            # Set to eval mode
            self.model.eval()
            
            self.is_loaded = True
            logger.info("‚úÖ Model ƒë∆∞·ª£c load th√†nh c√¥ng!")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi load model: {e}")
            self.is_loaded = False
            return False
    
    def is_available(self) -> bool:
        """Ki·ªÉm tra model c√≥ s·∫µn s√†ng kh√¥ng"""
        return self.is_loaded and self.model is not None and self.tokenizer is not None
    
    async def generate_response(
        self, 
        prompt: str, 
        max_length: int = 512, 
        temperature: float = 0.7,
        top_p: float = 0.9,
        do_sample: bool = True
    ) -> str:
        """Generate response t·ª´ prompt"""
        if not self.is_available():
            raise RuntimeError("Model ch∆∞a ƒë∆∞·ª£c load ho·∫∑c kh√¥ng kh·∫£ d·ª•ng")
        
        try:
            # Format prompt (c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh theo format c·ªßa Vintern)
            formatted_prompt = self._format_prompt(prompt)
            
            # Tokenize
            inputs = self.tokenizer(
                formatted_prompt,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            # Generate
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_length=max_length,
                    temperature=temperature,
                    top_p=top_p,
                    do_sample=do_sample,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id
                )
            
            # Decode response
            response = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi generate response: {e}")
            raise
    
    def _format_prompt(self, prompt: str) -> str:
        """Format prompt theo template c·ªßa Vintern model"""
        # Template c√≥ th·ªÉ c·∫ßn ƒëi·ªÅu ch·ªânh d·ª±a tr√™n documentation c·ªßa model
        return f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    async def analyze_image_with_context(
        self, 
        image_description: str, 
        detected_objects: list = None,
        user_question: str = None
    ) -> str:
        """Ph√¢n t√≠ch ·∫£nh v·ªõi context t·ª´ object detection v√† c√¢u h·ªèi c·ªßa user"""
        
        # T·∫°o prompt comprehensive
        prompt_parts = [f"Ph√¢n t√≠ch ·∫£nh n√†y: {image_description}"]
        
        if detected_objects:
            objects_info = ", ".join([obj.get('name', 'unknown') for obj in detected_objects])
            prompt_parts.append(f"C√°c v·∫≠t th·ªÉ ƒë∆∞·ª£c ph√°t hi·ªán: {objects_info}")
        
        if user_question:
            prompt_parts.append(f"C√¢u h·ªèi c·ª• th·ªÉ: {user_question}")
        
        prompt = "\n".join(prompt_parts)
        return await self.generate_response(prompt)
    
    def get_model_info(self) -> Dict[str, Any]:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            "model_path": self.model_path,
            "device": self.device,
            "is_loaded": self.is_loaded,
            "has_cuda": torch.cuda.is_available(),
            "model_name": "5CD-AI/Vintern-1B-v3_5"
        }

# Global instance
_local_model_instance: Optional[LocalVinternModel] = None

async def get_local_model() -> LocalVinternModel:
    """Get ho·∫∑c t·∫°o local model instance"""
    global _local_model_instance
    
    if _local_model_instance is None:
        _local_model_instance = LocalVinternModel()
        await _local_model_instance.initialize()
    
    return _local_model_instance