# ğŸ“ Raspberry Pi Integration Guide

HÆ°á»›ng dáº«n tÃ­ch há»£p Vision AI inference tá»« PC vÃ o Raspberry Pi 4 vá»›i backend FastAPI.

## ğŸ“‹ Prerequisites

### TrÃªn PC (Ä‘Ã£ setup):
- âœ… llama-server Ä‘ang cháº¡y port 8080
- âœ… Context size: 4096 tokens
- âœ… PC IP: `192.168.1.3` (hoáº·c IP thá»±c cá»§a báº¡n)

### TrÃªn Raspberry Pi 4:
- Python 3.8+
- Network káº¿t ná»‘i Ä‘Æ°á»£c vá»›i PC
- Backend FastAPI hiá»‡n cÃ³

## ğŸš€ Quick Start (5 phÃºt)

### BÆ°á»›c 1: Transfer Files Sang Pi

```bash
# TrÃªn PC, compress client code
cd /home/baobao/Projects/Vintern-1b-v3.5-demo
tar -czf pi-client.tar.gz client/ smart_analyze.py OUTPUT_LIMITATION.md

# Copy sang Pi (thay PI_IP báº±ng IP thá»±c cá»§a Pi)
scp pi-client.tar.gz pi@<PI_IP>:~/

# Hoáº·c dÃ¹ng USB náº¿u khÃ´ng cÃ³ SSH
```

### BÆ°á»›c 2: Setup TrÃªn Pi

```bash
# SSH vÃ o Pi
ssh pi@<PI_IP>

# Extract files
cd ~
tar -xzf pi-client.tar.gz

# Install dependencies
pip3 install requests pillow

# Test connection
cd client
python3 test_connection.py 192.168.1.3
```

Káº¿t quáº£ mong Ä‘á»£i:
```
âœ… PC Inference Server: AVAILABLE
ğŸŒ Network: OK (latency: 2.3ms)
ğŸ–¼ï¸ Vision Model: Vintern-1B-v3.5
```

### BÆ°á»›c 3: TÃ­ch Há»£p VÃ o Backend

Xem pháº§n [Backend Integration Examples](#-backend-integration-examples) bÃªn dÆ°á»›i.

---

## ğŸ¯ Backend Integration Examples

### Example 1: Simple Analysis (Fast)

**Use case:** MÃ´ táº£ nhanh má»™t frame tá»« camera

```python
# backend/app/services/vision_service.py

from client.pc_inference_client import PCInferenceClient
from PIL import Image
import io

class VisionService:
    def __init__(self):
        self.client = PCInferenceClient(
            host="192.168.1.3",  # PC IP
            port=8080,
            timeout=30
        )
    
    def analyze_frame(self, image_data: bytes) -> dict:
        """
        PhÃ¢n tÃ­ch nhanh má»™t frame
        
        Args:
            image_data: JPEG image bytes
            
        Returns:
            {"description": str, "tokens": int, "latency_ms": float}
        """
        import time
        
        start = time.time()
        
        # Convert bytes to PIL Image
        image = Image.open(io.BytesIO(image_data))
        
        # Simple question
        result = self.client.chat_completion(
            image=image,
            prompt="MÃ´ táº£ ngáº¯n gá»n nhá»¯ng gÃ¬ báº¡n tháº¥y trong áº£nh."
        )
        
        latency = (time.time() - start) * 1000
        
        return {
            "description": result.get("content", ""),
            "tokens": result.get("tokens", 0),
            "latency_ms": round(latency, 2)
        }
```

### Example 2: Detailed Analysis (Recommended â­)

**Use case:** PhÃ¢n tÃ­ch chi tiáº¿t cho object detection + description

```python
# backend/app/services/vision_detailed_service.py

from client.pc_inference_client import PCInferenceClient
from PIL import Image
import io

class DetailedVisionService:
    def __init__(self):
        self.client = PCInferenceClient(
            host="192.168.1.3",
            port=8080,
            timeout=60
        )
        
        # Multi-phase questions (giá»‘ng smart_analyze)
        self.analysis_phases = {
            "overview": [
                "Báº¡n tháº¥y gÃ¬ trong áº£nh nÃ y? MÃ´ táº£ ngáº¯n gá»n."
            ],
            "objects": [
                "CÃ³ nhá»¯ng loáº¡i váº­t thá»ƒ gÃ¬? Liá»‡t kÃª cá»¥ thá»ƒ.",
                "CÃ³ bao nhiÃªu váº­t thá»ƒ? Äáº¿m tá»«ng loáº¡i.",
            ],
            "colors": [
                "MÃ u sáº¯c cá»§a tá»«ng váº­t thá»ƒ nhÆ° tháº¿ nÃ o?"
            ],
            "layout": [
                "Váº­t thá»ƒ Ä‘Æ°á»£c sáº¯p xáº¿p nhÆ° tháº¿ nÃ o?",
                "Vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cá»§a cÃ¡c váº­t thá»ƒ ra sao?"
            ],
            "environment": [
                "Ná»n cá»§a áº£nh lÃ  gÃ¬? MÃ u gÃ¬?",
                "CÃ³ yáº¿u tá»‘ nÃ o khÃ¡c Ä‘Ã¡ng chÃº Ã½ khÃ´ng?"
            ]
        }
    
    def analyze_comprehensive(self, image_data: bytes) -> dict:
        """
        PhÃ¢n tÃ­ch toÃ n diá»‡n vá»›i multi-turn conversation
        
        Returns:
            {
                "summary": str,              # Tá»•ng há»£p táº¥t cáº£
                "phases": {...},             # Chi tiáº¿t tá»«ng phase
                "total_tokens": int,
                "total_time_ms": float
            }
        """
        import time
        
        start_time = time.time()
        image = Image.open(io.BytesIO(image_data))
        
        results = {}
        total_tokens = 0
        conversation_context = []
        
        for phase_name, questions in self.analysis_phases.items():
            phase_answers = []
            
            for i, question in enumerate(questions):
                # First question of first phase includes image
                if phase_name == "overview" and i == 0:
                    result = self.client.chat_completion(
                        image=image,
                        prompt=question,
                        context=conversation_context
                    )
                else:
                    # Subsequent questions use context (multi-turn)
                    result = self.client.chat_completion(
                        image=None,  # No image for follow-up
                        prompt=question,
                        context=conversation_context
                    )
                
                answer = result.get("content", "")
                phase_answers.append(answer)
                total_tokens += result.get("tokens", 0)
                
                # Update context for next question
                conversation_context = result.get("context", [])
            
            results[phase_name] = " ".join(phase_answers)
        
        # Generate comprehensive summary
        summary = " ".join(results.values())
        summary = " ".join(summary.split())  # Clean whitespace
        
        total_time = (time.time() - start_time) * 1000
        
        return {
            "summary": summary,
            "phases": results,
            "total_tokens": total_tokens,
            "total_time_ms": round(total_time, 2)
        }
```

### Example 3: With YOLO Detection Integration

**Use case:** Combine YOLO detection + VLM description

```python
# backend/app/services/integrated_vision_service.py

from client.pc_inference_client import PCInferenceClient
from .object_detection import YOLODetector  # Your existing YOLO
from PIL import Image
import io

class IntegratedVisionService:
    def __init__(self):
        self.vlm_client = PCInferenceClient(host="192.168.1.3", port=8080)
        self.yolo = YOLODetector()  # Your existing detector
    
    def analyze_with_detection(self, image_data: bytes) -> dict:
        """
        1. YOLO detect objects
        2. VLM verify vÃ  describe
        
        Returns:
            {
                "detections": [...],        # YOLO boxes
                "vlm_verification": str,    # VLM confirms what it sees
                "detailed_desc": str,       # Full description
                "confidence": float
            }
        """
        image = Image.open(io.BytesIO(image_data))
        
        # Step 1: YOLO detection
        detections = self.yolo.detect(image)
        
        # Step 2: Build smart prompt based on detections
        detected_labels = [d['label'] for d in detections]
        
        if detected_labels:
            # Ask VLM to verify YOLO results
            verify_prompt = (
                f"YOLO phÃ¡t hiá»‡n: {', '.join(detected_labels)}. "
                f"Báº¡n cÃ³ tháº¥y nhá»¯ng váº­t thá»ƒ nÃ y khÃ´ng? XÃ¡c nháº­n vÃ  mÃ´ táº£ chi tiáº¿t."
            )
        else:
            verify_prompt = "MÃ´ táº£ chi tiáº¿t nhá»¯ng gÃ¬ báº¡n tháº¥y trong áº£nh."
        
        # Step 3: Get VLM response
        result = self.vlm_client.chat_completion(
            image=image,
            prompt=verify_prompt
        )
        
        vlm_response = result.get("content", "")
        
        # Step 4: Calculate confidence (simple heuristic)
        confidence = self._calculate_confidence(detected_labels, vlm_response)
        
        return {
            "detections": detections,
            "vlm_verification": vlm_response,
            "confidence": confidence,
            "tokens": result.get("tokens", 0)
        }
    
    def _calculate_confidence(self, yolo_labels, vlm_text):
        """Simple matching between YOLO and VLM"""
        if not yolo_labels:
            return 0.5
        
        vlm_lower = vlm_text.lower()
        matches = sum(1 for label in yolo_labels if label.lower() in vlm_lower)
        
        return min(0.5 + (matches / len(yolo_labels)) * 0.5, 1.0)
```

---

## ğŸ”Œ FastAPI Endpoints

### ThÃªm vÃ o `backend/app/api/vision.py`:

```python
from fastapi import APIRouter, UploadFile, File, HTTPException
from ..services.vision_detailed_service import DetailedVisionService

router = APIRouter(prefix="/api/vision", tags=["vision"])

# Initialize service (singleton)
vision_service = DetailedVisionService()

@router.post("/analyze")
async def analyze_image(file: UploadFile = File(...)):
    """
    PhÃ¢n tÃ­ch chi tiáº¿t má»™t áº£nh
    
    POST /api/vision/analyze
    Body: multipart/form-data with 'file' field
    
    Returns:
        {
            "summary": "MÃ´ táº£ tá»•ng há»£p Ä‘áº§y Ä‘á»§...",
            "phases": {
                "overview": "...",
                "objects": "...",
                "colors": "...",
                "layout": "...",
                "environment": "..."
            },
            "total_tokens": 1234,
            "total_time_ms": 5678.9
        }
    """
    try:
        # Read image data
        image_data = await file.read()
        
        # Analyze
        result = vision_service.analyze_comprehensive(image_data)
        
        return {
            "success": True,
            "data": result
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/analyze/quick")
async def analyze_quick(file: UploadFile = File(...)):
    """
    PhÃ¢n tÃ­ch nhanh (1 cÃ¢u há»i duy nháº¥t)
    
    Use case: Real-time monitoring cáº§n response nhanh
    """
    from ..services.vision_service import VisionService
    
    try:
        image_data = await file.read()
        service = VisionService()
        result = service.analyze_frame(image_data)
        
        return {
            "success": True,
            "data": result
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/health")
async def check_vision_health():
    """
    Check xem PC inference server cÃ³ available khÃ´ng
    """
    try:
        status = vision_service.client.health_check()
        return {
            "success": True,
            "pc_server": status
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }
```

### Register trong `backend/app/main.py`:

```python
from fastapi import FastAPI
from .api import vision  # Import vision router

app = FastAPI(title="Vision AI Backend")

# Include vision endpoints
app.include_router(vision.router)

# ... existing routes ...
```

---

## ğŸ§ª Testing

### Test 1: PC Server Connection

```bash
# TrÃªn Pi
cd ~/client
python3 test_connection.py 192.168.1.3
```

### Test 2: Simple Analysis

```python
# test_simple.py
from client.pc_inference_client import PCInferenceClient
from PIL import Image

client = PCInferenceClient(host="192.168.1.3", port=8080)

# Test vá»›i má»™t áº£nh
image = Image.open("test.jpg")
result = client.chat_completion(
    image=image,
    prompt="MÃ´ táº£ áº£nh nÃ y"
)

print(result["content"])
```

### Test 3: API Endpoint

```bash
# Test FastAPI endpoint
curl -X POST http://localhost:8000/api/vision/analyze \
  -F "file=@test.jpg"
```

---

## ğŸ“Š Performance Tips

### 1. Cache VLM Client
```python
# Singleton pattern - khá»Ÿi táº¡o 1 láº§n duy nháº¥t
from functools import lru_cache

@lru_cache()
def get_vision_service():
    return DetailedVisionService()
```

### 2. Async Processing
```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

executor = ThreadPoolExecutor(max_workers=2)

async def analyze_async(image_data):
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(
        executor,
        vision_service.analyze_comprehensive,
        image_data
    )
```

### 3. Request Batching
```python
# Náº¿u cÃ³ nhiá»u frames cÃ¹ng lÃºc, batch chÃºng
async def batch_analyze(images: list):
    tasks = [analyze_async(img) for img in images]
    return await asyncio.gather(*tasks)
```

---

## ğŸ” Security & Best Practices

### 1. Validate Image Size
```python
MAX_IMAGE_SIZE = 10 * 1024 * 1024  # 10MB

if len(image_data) > MAX_IMAGE_SIZE:
    raise ValueError("Image too large")
```

### 2. Timeout Handling
```python
# ÄÃ£ cÃ³ sáºµn trong PCInferenceClient
client = PCInferenceClient(
    host="192.168.1.3",
    port=8080,
    timeout=30,  # 30 seconds
    max_retries=2
)
```

### 3. Error Handling
```python
try:
    result = client.chat_completion(image, prompt)
except ConnectionError:
    # PC server down
    return {"error": "Inference server unavailable"}
except TimeoutError:
    # Request quÃ¡ lÃ¢u
    return {"error": "Request timeout"}
```

---

## ğŸ“ˆ Monitoring

### Log Request Metrics
```python
import logging
import time

logger = logging.getLogger(__name__)

def analyze_with_logging(image_data):
    start = time.time()
    
    try:
        result = vision_service.analyze_comprehensive(image_data)
        
        latency = (time.time() - start) * 1000
        
        logger.info(
            f"Vision analysis completed: "
            f"tokens={result['total_tokens']}, "
            f"latency={latency:.2f}ms"
        )
        
        return result
        
    except Exception as e:
        logger.error(f"Vision analysis failed: {e}")
        raise
```

---

## ğŸ› ï¸ Troubleshooting

### PC Server Connection Refused
```bash
# Check PC server
curl http://192.168.1.3:8080/health

# Check firewall
sudo ufw allow 8080/tcp

# Restart server náº¿u cáº§n
cd /home/baobao/Projects/Vintern-1b-v3.5-demo/pc-inference-server
./start_server.sh
```

### Slow Response Time
```python
# Enable verbose logging
client = PCInferenceClient(host="192.168.1.3", port=8080)
client.enable_debug()  # Shows timing for each step
```

### Context Memory Issues
```python
# Reset context sau má»—i frame Ä‘á»ƒ trÃ¡nh memory leak
conversation_context = []

for frame in video_stream:
    result = client.chat_completion(
        image=frame,
        prompt="...",
        context=conversation_context  # Reuse within session
    )
    
    # Reset má»—i 10 frames
    if frame_count % 10 == 0:
        conversation_context = []
```

---

## ğŸ“š Next Steps

1. âœ… Test connection giá»¯a Pi vÃ  PC
2. âœ… TÃ­ch há»£p vÃ o 1 endpoint FastAPI (quick analyze)
3. âœ… Test vá»›i RTSP stream tháº­t
4. âœ… ThÃªm detailed analysis endpoint
5. âš™ï¸ Optimize performance (caching, batching)
6. ğŸ“Š Add monitoring/logging
7. ğŸš€ Deploy to production

---

## ğŸ”— Related Documentation

- [ARCHITECTURE.md](ARCHITECTURE.md) - System overview
- [OUTPUT_LIMITATION.md](OUTPUT_LIMITATION.md) - Model limitations
- [client/README.md](client/README.md) - Client library API
- [pc-inference-server/README.md](pc-inference-server/README.md) - Server setup

---

**Happy Coding! ğŸ‰** CÃ³ váº¥n Ä‘á» gÃ¬ cá»© há»i nhÃ©!
