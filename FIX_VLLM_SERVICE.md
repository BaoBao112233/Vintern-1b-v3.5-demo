# H∆∞·ªõng d·∫´n Fix VLLM Service (Orange Pi)

## V·∫•n ƒë·ªÅ hi·ªán t·∫°i
VLLM service ƒëang ch·∫°y ·ªü ch·∫ø ƒë·ªô **proxy mode** v√† g√¢y ra circular dependency:
```
Backend (192.168.1.14:8000) ‚Üí VLLM (192.168.1.16:8002) ‚Üí Backend (192.168.1.14:8000) ‚ùå
```

Response t·ª´ VLLM:
```json
{
  "success": true,
  "response": "Backend service unavailable. Please ensure the inference service is running at http://192.168.1.14:8000"
}
```

## Gi·∫£i ph√°p

### B∆∞·ªõc 1: SSH v√†o Orange Pi
```bash
ssh orangepi@192.168.1.16
# Nh·∫≠p password c·ªßa orangepi user
```

### B∆∞·ªõc 2: T√¨m VLLM service
```bash
# T√¨m th∆∞ m·ª•c VLLM service
find ~/ -name "vllm-service" -type d 2>/dev/null
ls -la ~/vllm-service/ || ls -la ~/Projects/ | grep vllm

# Ki·ªÉm tra process ƒëang ch·∫°y
ps aux | grep -E "python.*8002|uvicorn.*8002" | grep -v grep
```

### B∆∞·ªõc 3: Ki·ªÉm tra c·∫•u h√¨nh hi·ªán t·∫°i
```bash
cd ~/vllm-service  # ho·∫∑c ƒë∆∞·ªùng d·∫´n b·∫°n t√¨m ƒë∆∞·ª£c

# Xem file c·∫•u h√¨nh
cat .env
cat app/main.py | grep -A10 "mode\|proxy\|backend"
```

### B∆∞·ªõc 4: S·ª≠a c·∫•u h√¨nh
T√¨m v√† s·ª≠a file `.env` ho·∫∑c file c·∫•u h√¨nh ch√≠nh:

**T√åM:**
```bash
MODE=proxy
BACKEND_URL=http://192.168.1.14:8000
```

**ƒê·ªîI TH√ÄNH:**
```bash
MODE=vllm  # ho·∫∑c MODE=direct
# Comment ho·∫∑c x√≥a d√≤ng BACKEND_URL
# BACKEND_URL=http://192.168.1.14:8000
```

### B∆∞·ªõc 5: ƒê·∫£m b·∫£o model ƒë∆∞·ª£c load
```bash
# Ki·ªÉm tra model path
echo $MODEL_PATH
ls -la models/ || ls -la ~/.cache/huggingface/hub/

# N·∫øu ch∆∞a c√≥ model, download:
python3 download_model.py  # n·∫øu c√≥ script
# ho·∫∑c
huggingface-cli download 5CD-AI/Vintern-1B-v3_5
```

### B∆∞·ªõc 6: Restart VLLM service
```bash
# Stop service
pkill -f "uvicorn.*8002" || pkill -f "python.*8002"

# Start l·∫°i
cd ~/vllm-service
source venv/bin/activate  # n·∫øu d√πng virtualenv
python3 -m uvicorn app.main:app --host 0.0.0.0 --port 8002 --reload

# Ho·∫∑c n·∫øu c√≥ script startup:
./start.sh
```

### B∆∞·ªõc 7: Ki·ªÉm tra t·ª´ Raspberry Pi
```bash
# T·ª´ Raspberry Pi, test VLLM service
curl -s http://192.168.1.16:8002/health | python3 -m json.tool

# Test analyze endpoint
curl -s -X POST http://192.168.1.16:8002/api/analyze \
  -H "Content-Type: application/json" \
  -d '{
    "image_description":"Test camera",
    "detected_objects":[{"label":"person","confidence":0.9}],
    "question":"What do you see?"
  }' | python3 -m json.tool
```

K·∫øt qu·∫£ mong ƒë·ª£i:
```json
{
  "success": true,
  "response": "I can see a person in the image...",  // ‚Üê Ph·∫£n h·ªìi th·ª±c t·ª´ model
  "model_info": {
    "mode": "vllm",  // ‚Üê Kh√¥ng c√≤n "proxy"
    "is_loaded": true
  }
}
```

## Alternative: N·∫øu kh√¥ng th·ªÉ SSH

### Option A: Restart t·ª´ xa (n·∫øu c√≥ systemd)
```bash
# T·ª´ Raspberry Pi
ssh orangepi@192.168.1.16 "systemctl --user restart vllm-service"
```

### Option B: T·∫°m th·ªùi disable VLLM v√† d√πng detection only
Trong file `.env` c·ªßa Raspberry Pi:
```bash
# Comment out VLLM service
# VLLM_SERVICE_URL=http://192.168.1.16:8002
```

Restart backend:
```bash
sudo systemctl restart vintern-backend
# ho·∫∑c
pkill -f "uvicorn.*8000"
cd ~/Projects/Vintern-1b-v3.5-demo/backend
python3 -m uvicorn app.main:app --host 0.0.0.0 --port 8000
```

## Ki·ªÉm tra sau khi fix

1. **Health check:**
```bash
curl http://localhost:8000/api/health | python3 -m json.tool
```

K·ª≥ v·ªçng th·∫•y:
```json
{
  "vllm_ready": true,
  "vllm_info": {
    "vllm_url": "http://192.168.1.16:8002",
    "available": true,
    "status": "ready"
  }
}
```

2. **Test qua web interface:**
- M·ªü http://192.168.1.14:8000/
- Hard refresh (Ctrl+Shift+R)
- B·∫≠t "ü§ñ Continuous AI Analysis"
- Ki·ªÉm tra status bar: "VLLM AI" ph·∫£i hi·ªán m√†u xanh ‚úÖ
- Click "Analyze with AI" tr√™n m·ªói camera
- Xem response trong ph·∫ßn "AI Analysis"

## Troubleshooting

### VLLM kh√¥ng start ƒë∆∞·ª£c
```bash
# Ki·ªÉm tra logs
journalctl -u vllm-service -f  # n·∫øu d√πng systemd
# ho·∫∑c
tail -f ~/vllm-service/logs/*.log
```

### Out of memory
```bash
# Ki·ªÉm tra RAM
free -h
# N·∫øu thi·∫øu RAM, d√πng quantized model ho·∫∑c gi·∫£m context length
```

### Model kh√¥ng load ƒë∆∞·ª£c
```bash
# Verify model files
ls -lh ~/.cache/huggingface/hub/models--5CD-AI--Vintern-1B-v3_5/

# Re-download n·∫øu c·∫ßn
rm -rf ~/.cache/huggingface/hub/models--5CD-AI--Vintern-1B-v3_5/
huggingface-cli download 5CD-AI/Vintern-1B-v3_5
```

## Li√™n h·ªá
N·∫øu v·∫´n g·∫∑p v·∫•n ƒë·ªÅ, c·∫ßn th√¥ng tin sau ƒë·ªÉ debug:
1. Output c·ªßa `ps aux | grep 8002`
2. N·ªôi dung file `.env` c·ªßa VLLM service
3. Logs t·ª´ VLLM service
4. Hardware info: `free -h && df -h`
